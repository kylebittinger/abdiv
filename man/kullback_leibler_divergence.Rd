% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/beta.R
\name{kullback_leibler_divergence}
\alias{kullback_leibler_divergence}
\title{Kullback-Leibler divergence}
\usage{
kullback_leibler_divergence(x, y)
}
\arguments{
\item{x, y}{Numeric vectors representing probabilities}
}
\description{
Kullback-Leibler divergence
}
\details{
Kullback-Leibler divergence is a non-symmetric measure of difference between
two probability vectors. In general, KL(x, y) is not equal to KL(y, x).

Because this measure is defined for probabilities, the vectors x and y are
normalized in the function so they sum to 1.

The Kullback-Leibler divergence is not defined when y_i == 0 but x_i > 0. In
this case, the function returns NaN.
}
